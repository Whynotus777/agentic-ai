# monitoring/alerts.yaml
# EXTENDED: Added alert owners, labels, and paging policies

# Paging Policy Reference:
# P0: Immediate page (24/7) - Production down
# P1: Page during business hours, notify off-hours - Service degradation
# P2: Notify during business hours - Performance issues
# P3: Daily digest - Capacity planning

groups:
  - name: error_budget_alerts
    interval: 30s
    rules:
      # (a) Tool error rate alert
      - alert: HighToolErrorRate
        expr: |
          (
            sum by (service, capability, tenant) (
              rate(tool_invocation_errors_total[10m])
            ) /
            sum by (service, capability, tenant) (
              rate(tool_invocation_total[10m])
            )
          ) > 0.05
        for: 10m
        labels:
          severity: critical
          owner: "platform-team@company.com"
          paging_policy: "P1"
          service: "{{ $labels.service }}"
          capability: "{{ $labels.capability }}"
          tenant: "{{ $labels.tenant }}"
          team: platform
          slo_type: tool_reliability
        annotations:
          summary: "Tool error rate >5% for {{ $labels.service }}/{{ $labels.capability }}"
          description: |
            Service: {{ $labels.service }}
            Capability: {{ $labels.capability }}
            Tenant: {{ $labels.tenant }}
            Current error rate: {{ $value | humanizePercentage }}
            Threshold: 5%
            Duration: 10 minutes
          runbook_url: "https://wiki.internal/runbooks/tool-errors"
          dashboard_url: "https://grafana.internal/d/tool-errors?var-service={{ $labels.service }}"
          owner_slack: "#platform-oncall"

      # (c) Stage SLO breaches
      - alert: PlanStageSLOBreach
        expr: |
          (
            sum by (service, capability, tenant) (
              rate(stage_errors_total{stage="plan"}[10m])
            ) /
            sum by (service, capability, tenant) (
              rate(stage_requests_total{stage="plan"}[10m])
            )
          ) > 0.02
        for: 10m
        labels:
          severity: warning
          owner: "orchestration-team@company.com"
          paging_policy: "P2"
          service: "{{ $labels.service }}"
          capability: "{{ $labels.capability }}"
          tenant: "{{ $labels.tenant }}"
          stage: plan
          slo_type: planning_reliability
        annotations:
          summary: "Planning stage SLO breach for {{ $labels.service }}"
          description: |
            Stage: Planning
            Service: {{ $labels.service }}
            Error rate: {{ $value | humanizePercentage }} (threshold: 2%)
          owner_slack: "#orchestration-team"

      - alert: ToolStageSLOBreach
        expr: |
          (
            sum by (service, capability, tenant) (
              rate(stage_errors_total{stage="tool"}[10m])
            ) /
            sum by (service, capability, tenant) (
              rate(stage_requests_total{stage="tool"}[10m])
            )
          ) > 0.05
        for: 10m
        labels:
          severity: warning
          owner: "platform-team@company.com"
          paging_policy: "P2"
          service: "{{ $labels.service }}"
          capability: "{{ $labels.capability }}"
          tenant: "{{ $labels.tenant }}"
          stage: tool
          slo_type: tool_execution
        annotations:
          summary: "Tool stage SLO breach for {{ $labels.service }}"
          description: |
            Stage: Tool Execution
            Service: {{ $labels.service }}
            Error rate: {{ $value | humanizePercentage }} (threshold: 5%)
          owner_slack: "#platform-oncall"

      - alert: SynthStageSLOBreach
        expr: |
          (
            sum by (service, capability, tenant) (
              rate(stage_errors_total{stage="synth"}[10m])
            ) /
            sum by (service, capability, tenant) (
              rate(stage_requests_total{stage="synth"}[10m])
            )
          ) > 0.01
        for: 10m
        labels:
          severity: warning
          owner: "ml-team@company.com"
          paging_policy: "P2"
          service: "{{ $labels.service }}"
          capability: "{{ $labels.capability }}"
          tenant: "{{ $labels.tenant }}"
          stage: synth
          slo_type: synthesis_reliability
        annotations:
          summary: "Synthesis stage SLO breach for {{ $labels.service }}"
          description: |
            Stage: Synthesis
            Service: {{ $labels.service }}
            Error rate: {{ $value | humanizePercentage }} (threshold: 1%)
          owner_slack: "#ml-platform"

  - name: cost_anomaly_alerts
    interval: 60s
    rules:
      # (b) Daily cost anomaly detection
      - alert: DailyCostAnomaly
        expr: |
          (
            sum by (service, capability, tenant) (
              increase(span_cost_usd_total[1d])
            ) >
            2 * avg_over_time(
              sum by (service, capability, tenant) (
                increase(span_cost_usd_total[1d])
              )[7d:1d]
            )
          )
        for: 30m
        labels:
          severity: warning
          owner: "finops-team@company.com"
          paging_policy: "P3"
          service: "{{ $labels.service }}"
          capability: "{{ $labels.capability }}"
          tenant: "{{ $labels.tenant }}"
          team: finops
          category: cost_control
        annotations:
          summary: "Daily cost >2x 7-day avg for {{ $labels.service }}/{{ $labels.tenant }}"
          description: |
            Service: {{ $labels.service }}
            Capability: {{ $labels.capability }}
            Tenant: {{ $labels.tenant }}
            Current daily cost: ${{ $value | humanize }}
            7-day average: ${{ $labels.avg_cost | default "N/A" }}
            Threshold: 2x average over 30 minutes
          runbook_url: "https://wiki.internal/runbooks/cost-anomaly"
          cost_dashboard: "https://grafana.internal/d/finops?var-tenant={{ $labels.tenant }}"
          owner_slack: "#finops-alerts"

      - alert: HourlyCostSpike
        expr: |
          (
            sum by (service, capability, tenant) (
              rate(span_cost_usd_total[1h])
            ) >
            3 * avg_over_time(
              sum by (service, capability, tenant) (
                rate(span_cost_usd_total[1h])
              )[24h:1h]
            )
          )
        for: 15m
        labels:
          severity: warning
          owner: "finops-team@company.com"
          paging_policy: "P2"
          service: "{{ $labels.service }}"
          capability: "{{ $labels.capability }}"
          tenant: "{{ $labels.tenant }}"
          team: finops
          category: cost_control
        annotations:
          summary: "Hourly cost spike (3x normal) for {{ $labels.tenant }}"
          description: |
            Service: {{ $labels.service }}
            Current hourly rate: ${{ $value | humanize }}/hr
            Normal rate: ~${{ $labels.normal_rate | default "N/A" }}/hr
          owner_slack: "#finops-alerts"

      - alert: TenantCostThreshold
        expr: |
          sum by (tenant) (increase(span_cost_usd_total[1h])) > 100
        for: 10m
        labels:
          severity: info
          owner: "customer-success@company.com"
          paging_policy: "P3"
          tenant: "{{ $labels.tenant }}"
          team: customer_success
        annotations:
          summary: "Tenant {{ $labels.tenant }} exceeds $100/hr"
          description: |
            Tenant: {{ $labels.tenant }}
            Current hourly cost: ${{ $value | humanize }}
            This may trigger billing alerts for the customer
          owner_slack: "#customer-success"

  - name: security_alerts
    interval: 30s
    rules:
      - alert: PolicyBypassAttempt
        expr: |
          sum by (service, user, action) (
            increase(policy_bypass_attempts_total[5m])
          ) > 0
        for: 1m
        labels:
          severity: critical
          owner: "security-team@company.com"
          paging_policy: "P0"
          service: "{{ $labels.service }}"
          user: "{{ $labels.user }}"
          action: "{{ $labels.action }}"
          team: security
          siem_forward: "true"
        annotations:
          summary: "Policy bypass attempt by {{ $labels.user }}"
          description: |
            User: {{ $labels.user }}
            Service: {{ $labels.service }}
            Action attempted: {{ $labels.action }}
            Count in last 5m: {{ $value }}
            SECURITY INCIDENT - Forwarded to SIEM
          incident_response: "https://wiki.internal/security/bypass-response"
          owner_slack: "#security-oncall"
          siem_event_id: "SEC-{{ $labels.service }}-{{ $labels.user }}"

      - alert: UnauthorizedDataAccess
        expr: |
          sum by (service, capability, tenant, data_classification) (
            increase(unauthorized_access_total[5m])
          ) > 0
        for: 1m
        labels:
          severity: critical
          owner: "security-team@company.com"
          paging_policy: "P0"
          service: "{{ $labels.service }}"
          capability: "{{ $labels.capability }}"
          tenant: "{{ $labels.tenant }}"
          data_classification: "{{ $labels.data_classification }}"
          team: security
          siem_forward: "true"
        annotations:
          summary: "Unauthorized {{ $labels.data_classification }} access in {{ $labels.service }}"
          description: |
            Service: {{ $labels.service }}
            Tenant: {{ $labels.tenant }}
            Data Classification: {{ $labels.data_classification }}
            Attempts: {{ $value }}
          owner_slack: "#security-oncall"

# Alert routing configuration (for Alertmanager)
routing_config:
  routes:
    - match:
        paging_policy: P0
      receiver: pagerduty_critical
      continue: true
    - match:
        paging_policy: P1
      receiver: pagerduty_high
      continue: true
    - match:
        paging_policy: P2
      receiver: slack_notifications
      continue: true
    - match:
        paging_policy: P3
      receiver: email_daily
    - match:
        siem_forward: "true"
      receiver: siem_webhook

  receivers:
    - name: pagerduty_critical
      webhook_configs:
        - url: "https://events.pagerduty.com/v2/enqueue"
    - name: pagerduty_high
      webhook_configs:
        - url: "https://events.pagerduty.com/v2/enqueue"
    - name: slack_notifications
      slack_configs:
        - api_url: "${SLACK_WEBHOOK_URL}"
    - name: email_daily
      email_configs:
        - to: "oncall-digest@company.com"
    - name: siem_webhook
      webhook_configs:
        - url: "https://siem-stub.internal/webhook"